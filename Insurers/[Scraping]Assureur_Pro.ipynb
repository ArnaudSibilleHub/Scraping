{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests as rq \n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_list = rq.get('https://www.assureurs.pro/entreprise_localisation/toutes-les-regions/nord-pas-de-calais/')\n",
    "raw_list.status_code\n",
    "print(raw_list)\n",
    "list_page = raw_list.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup\n",
    "list_bsp = BeautifulSoup(list_page, 'html.parser')\n",
    "list_bsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPING URLS SUBPAGES NORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "('https://www.assureurs.pro/entreprise_localisation/toutes-les-regions/nord-pas-de-calais/nord/')\n",
    "\n",
    "# Find all \"a\" elements on the page\n",
    "list_links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "# Create an empty list to store the href attributes\n",
    "list_url = []\n",
    "\n",
    "# Iterate through each anchor element\n",
    "\n",
    "for link in list_links:\n",
    "    # Get the href attribute and append it to the list_url\n",
    "    href = link.get_attribute('href')\n",
    "    if href == None or link == None :\n",
    "        continue\n",
    "    else :\n",
    "        list_url.append(href)\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Print or use the list_url as needed\n",
    "print(list_url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_url_pd = pd.DataFrame({'URL': list_url})\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = 'output_urls_AllNord.csv'\n",
    "list_url_pd.to_csv(csv_filename, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPINGS SUBPAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_insurers_lists_as_dataframe(url):\n",
    "    try:\n",
    "        #only apply this function to url who contains adresses \n",
    "        #https://www.assureurs.pro/entreprise_localisation/toutes-les-regions/nord-pas-de-calais/nord/\n",
    "        #optionally followed by all of a literal /, optionally followed by a literal home, optionally followed by a literal ? and any number of other characters.\n",
    "        patternUrl = r\"^https:\\/\\/www\\.assureurs\\.pro\\/entreprise_localisation\\/toutes-les-regions\\/nord-pas-de-calais\\/nord\\/([^']+)\\/\"\n",
    "        matchUrl = re.search(patternUrl, url)\n",
    "        if matchUrl:\n",
    "            page = rq.get(url)\n",
    "            page.raise_for_status()\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            #extract artcile within targetted url\n",
    "            \n",
    "            elements = soup.find_all('article', onclick=True)\n",
    "            print(elements)\n",
    "            \n",
    "\n",
    "            # Extract the URL using regular expression\n",
    "            if elements:\n",
    "                for element in elements:\n",
    "                    onclick_value = element['onclick'] #following BS list created including onclick\n",
    "                    # Use re.search to find the pattern in the onclick_value\n",
    "                    pattern = r\"window\\.location\\.href\\s*=\\s*'([^']+)'\" \n",
    "                    match = re.search(pattern, onclick_value)\n",
    "                    \n",
    "                    \n",
    "                    if match:\n",
    "                        url2 = match.group(1) #(1)?\n",
    "                        print(f'Extracted URL: {url2}')\n",
    "                    # create a dic with basic info (.text.strip())\n",
    "\n",
    "                        \n",
    "                        #info = {\n",
    "                        #'onclick': url2,\n",
    "                        #'name': [item.text.strip() if item else \"Aucun titre\" for item in soup.select('.tile.tile-entreprise a')],\n",
    "                        #'address': [item.text.strip() if item else \"Aucune description\" for item in soup.select('.tile.tile-entreprise .tile__body')],\n",
    "                        #}\n",
    "\n",
    "                        #'onclick': url2,\n",
    "                        #'name': soup.select('.tile.tile-entreprise a') if soup.select('.tile.tile-entreprise a') else \"Aucun titre\",\n",
    "                        #'adress': soup.select('.tile.tile-entreprise .tile__body') if soup.select('.tile.tile-entreprise .tile__body') else \"Aucune adresse\"\n",
    "                    \n",
    "                    #data.append(info)\n",
    "\n",
    "                    # Select all elements \"div\" inside \"articles\" that match the specified class within information\n",
    "                        tiles = soup.select('.tile.tile-entreprise')\n",
    "\n",
    "                        # Create a list to store dictionaries\n",
    "                        text_div_list = []\n",
    "\n",
    "                    # Loop through each tile element and extract data\n",
    "                    for index, tile in enumerate(tiles):\n",
    "                        onclick_value = url2\n",
    "                        name_values = [a.text for a in tile.select('a')] or \"Aucun titre\"\n",
    "                        address_values = [body.text for body in tile.select('.tile__body')] or \"Aucune adresse\"\n",
    "\n",
    "                        # Fill dictionary for the current set of data\n",
    "                        data_dict = {\n",
    "                            'url': onclick_value,\n",
    "                            'name': name_values,\n",
    "                            'address': address_values\n",
    "                        }\n",
    "\n",
    "                        # Append the dictionary to the list\n",
    "                        text_div_list.append(data_dict)\n",
    "\n",
    "                        \n",
    "                        # Display the resulting list of dictionaries\n",
    "                        for data_dict in text_div_list:\n",
    "                            print(data_dict)\n",
    "\n",
    "                    else: #(if no match)\n",
    "                        print('No URL found in onclick attribute.')\n",
    "                        continue\n",
    "                    \n",
    "                df = pd.DataFrame(text_div_list) \n",
    "                return df  \n",
    "            else:   \n",
    "                print('Element with onclick attribute not found.')\n",
    "        else: \n",
    "            print('No matched URL')\n",
    "    except rq.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération de la page : {e}\")\n",
    "        #return pd.DataFrame()\n",
    "\n",
    "# path CSV file including urls\n",
    "file_path = 'C:\\\\Users\\\\BGE\\\\ownCloud\\\\Arnaud\\\\DEVELOPPEMENT\\\\ACTIVITE\\\\Sponso owncloud perso AS\\\\Scraping\\\\Assu_Nord_012024\\\\output_urls_AllNord.csv'\n",
    "\n",
    "# Read CSV file including urls\n",
    "urls_df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Apply function on every URL and concatenate résults\n",
    "all_data_frames = [extract_insurers_lists_as_dataframe(url) for url in urls_df[1]]\n",
    "print(all_data_frames)\n",
    "\n",
    "result_df = pd.concat(all_data_frames, ignore_index=False)\n",
    "\n",
    "# Afficher ou sauvegarder le DataFrame résultant\n",
    "result_df\n",
    "# result_df.to_csv('chemin_pour_sauvegarder.csv')  # Décommenter pour sauvegarder dans un fichier CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean datas and split columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "insurers_list = result_df.applymap(lambda x: ''.join(x).strip())\n",
    "\n",
    "#splitting \n",
    "# Create 'insurers_list_addresses' DataFrame\n",
    "insurers_list_addresses = pd.DataFrame()\n",
    "insurers_list_addresses[['street', 'zipcode and city']] = insurers_list['address'].str.split('\\n', expand=True)\n",
    "\n",
    "insurers_list_cities= pd.DataFrame()\n",
    "insurers_list_cities[['zipcode','city']] = insurers_list_addresses['zipcode and city'].str.split(' ', expand=True)\n",
    "insurers_list_cities\n",
    "\n",
    "#concatening\n",
    "insurers= pd.DataFrame()\n",
    "insurers = [insurers_list, insurers_list_addresses, insurers_list_cities]\n",
    "insurers_list_vf = pd.concat(insurers, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index with the number of the line as a unique identifier\n",
    "insurers_list_vf.index = range(1, len(insurers_list_vf) + 1)\n",
    "insurers_list_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurers_list_vf = insurers_list_vf[:][['url', 'name', 'street', 'zipcode', 'city']]\n",
    "insurers_list_vf = insurers_list_vf.rename_axis('ID')\n",
    "insurers_list_vf.sort_values('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase \"None\" address \n",
    "import numpy as np\n",
    "insurers_list_vf['street'].replace('Aucune adresse', np.nan)\n",
    "insurers_list_vf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurers_list_vf.groupby('city').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.applymap(lambda x: str(x).strip())\n",
    "csv_filename = 'outputs_lists_insurers_AllNord.csv'\n",
    "insurers_list_vf.to_csv(csv_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELENIUM / WEBDRIVER - send form response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webdriverwait and Explicit COnditions \n",
    "\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the WebDriver (adjust the path based on your browser)\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "service = Service(executable_path='C:\\\\Users\\\\BGE\\\\chromedriver_win32\\\\chromedriver.exe')\n",
    "\n",
    "#instance webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "#requests looping list of pages including forms\n",
    "urls_forms = list(insurers_list_vf['url'])\n",
    "for url_forms in urls_forms:\n",
    "    driver.get(url_forms)\n",
    "\n",
    "    # Use WebDriverWait to wait for the form elements to be present \n",
    "    try:\n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            # box welcoming text to express the need\n",
    "            EC.presence_of_element_located((By.NAME, 'input_9')),\n",
    "            #firstname\n",
    "            EC.presence_of_element_located((By.NAME, 'input_11')),\n",
    "            #name\n",
    "            EC.presence_of_element_located((By.NAME, 'input_10')),\n",
    "            #address\n",
    "            EC.presence_of_element_located((By.NAME, 'input_2.1')),\n",
    "            #zipcode\n",
    "            EC.presence_of_element_located((By.NAME, 'input_2.5')),\n",
    "            #city\n",
    "            EC.presence_of_element_located((By.NAME, 'input_2.3')),\n",
    "            #email\n",
    "            EC.presence_of_element_located((By.NAME, 'input_3')),\n",
    "            #phone\n",
    "            EC.presence_of_element_located((By.NAME, 'input_4'))\n",
    "        )\n",
    "\n",
    "\n",
    "        # Fill the form (replace with the actual form elements and data)\n",
    "        driver.find_element(By.NAME, 'input_9').send_keys('Your data')\n",
    "        driver.find_element(By.NAME, 'input_11').send_keys('Your data')\n",
    "        driver.find_element(By.NAME, 'input_10').send_keys('Your data')\n",
    "        driver.find_element(By.NAME, 'input_2.1').send_keys('Your data')\n",
    "        driver.find_element(By.NAME, 'input_2.5').send_keys('Your data')\n",
    "        driver.find_element(By.NAME, 'input_2.3').send_keys('Your data')\n",
    "        driver.find_element(By.NAME, 'input_3').send_keys('Your data')\n",
    "        driver.find_element(By.NAME, 'input_4').send_keys('Your data')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        continue  # Move to the next URL if an error occurs\n",
    "\n",
    "    # Fill the form (replace with the actual form elements and data)\n",
    "    driver.find_element(By.NAME, 'input_9').send_keys('Your data')\n",
    "    driver.find_element(By.NAME, 'input_11').send_keys('Your data')\n",
    "    driver.find_element(By.NAME, 'input_10').send_keys('Your data')\n",
    "    driver.find_element(By.NAME, 'input_2.1').send_keys('Your data')\n",
    "    driver.find_element(By.NAME, 'input_2.5').send_keys('Your data')\n",
    "    driver.find_element(By.NAME, 'input_2.3').send_keys('Your data')\n",
    "    driver.find_element(By.NAME, 'input_3').send_keys('Your data')\n",
    "    driver.find_element(By.NAME, 'input_4').send_keys('Your data')\n",
    "\n",
    "    # Submit the form (replace with the actual submit action)\n",
    "    #driver.find_element(By.ID, 'gform_submit_button_1').click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
